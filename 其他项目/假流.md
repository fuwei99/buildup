你好！我已经为你梳理了该项目中关于“假流式”（Fake Streaming）的实现逻辑。

**核心原理：**
假流式的核心在于**“心跳机制”**。由于后端需要等待完整的非流式响应（可能耗时较长），为了防止客户端（如浏览器或API调用方）因长时间无数据传输而超时断开连接，服务端会先建立流式连接，并定期发送空的“心跳包”数据，直到真正的响应生成完毕，再一次性将结果作为流式数据块发送给客户端。

以下是摘抄的相关代码段：

### 1. Gemini 格式假流式实现
**文件：** `src/gemini_router.py`

这段代码展示了如何处理 Gemini 格式的假流式。它每隔 3 秒发送一次空内容的 `candidates` 作为心跳。

```python
async def fake_stream_response_gemini(request_data: dict, model: str):
    """处理Gemini格式的假流式响应"""

    async def gemini_stream_generator():
        try:
            # ... (省略凭证获取和Payload构建代码) ...

            # 1. 定义心跳包结构 (空内容)
            heartbeat = {
                "candidates": [
                    {
                        "content": {"parts": [{"text": ""}], "role": "model"},
                        "finishReason": None,
                        "index": 0,
                    }
                ]
            }
            # 发送首次心跳
            yield f"data: {json.dumps(heartbeat)}\n\n".encode()

            # 2. 异步发送实际请求 (在后台运行)
            async def get_response():
                return await send_gemini_request(api_payload, False, cred_mgr)

            # create_managed_task 是项目封装的 asyncio.create_task
            response_task = create_managed_task(get_response(), name="gemini_fake_stream_request")

            try:
                # 3. 循环发送心跳，直到任务完成
                # 每3秒发送一次心跳，防止客户端超时
                while not response_task.done():
                    await asyncio.sleep(3.0)
                    if not response_task.done():
                        yield f"data: {json.dumps(heartbeat)}\n\n".encode()

                # 获取响应结果
                response = await response_task

            except asyncio.CancelledError:
                response_task.cancel()
                raise
            
            # ... (省略错误处理代码) ...

            # 4. 处理并发送最终结果
            # (解析 response_data ...)
            
            # 发送完整内容作为单个chunk
            if "candidates" in response_data and response_data["candidates"]:
                candidate = response_data["candidates"][0]
                # ... (省略内容提取逻辑) ...
                
                content_chunk = {
                    "candidates": [
                        {
                            "content": {"parts": parts_response, "role": "model"},
                            "finishReason": candidate.get("finishReason", "STOP"),
                            "index": 0,
                        }
                    ]
                }
                yield f"data: {json.dumps(content_chunk)}\n\n".encode()
            
            # 5. 发送结束标记
            yield "data: [DONE]\n\n".encode()

        except Exception as e:
            # ... (错误处理) ...
            yield "data: [DONE]\n\n".encode()

    return StreamingResponse(gemini_stream_generator(), media_type="text/event-stream")
```

### 2. OpenAI 格式假流式实现
**文件：** `src/openai_router.py`

OpenAI 格式的逻辑基本一致，只是心跳包的数据结构不同（使用 `choices[0].delta`）。

```python
async def fake_stream_response(api_payload: dict, cred_mgr: CredentialManager) -> StreamingResponse:
    """处理OpenAI格式的假流式响应"""

    async def stream_generator():
        try:
            # 1. 定义心跳包结构 (空 delta)
            heartbeat = {
                "choices": [
                    {
                        "index": 0,
                        "delta": {"role": "assistant", "content": ""},
                        "finish_reason": None,
                    }
                ]
            }
            yield f"data: {json.dumps(heartbeat)}\n\n".encode()

            # 2. 异步发送实际请求
            async def get_response():
                return await send_gemini_request(api_payload, False, cred_mgr)

            response_task = create_managed_task(get_response(), name="openai_fake_stream_request")

            try:
                # 3. 循环发送心跳
                while not response_task.done():
                    await asyncio.sleep(3.0)
                    if not response_task.done():
                        yield f"data: {json.dumps(heartbeat)}\n\n".encode()

                # 获取响应结果
                response = await response_task

            except asyncio.CancelledError:
                response_task.cancel()
                raise

            # ... (省略响应解析代码) ...

            # 4. 构建并发送最终结果 Chunk
            if content:
                delta = {"role": "assistant", "content": content}
                # 如果有思维链内容，也一并放入
                if reasoning_content:
                    delta["reasoning_content"] = reasoning_content

                content_chunk = {
                    "id": str(uuid.uuid4()),
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": "gcli2api-streaming",
                    "choices": [{"index": 0, "delta": delta, "finish_reason": "stop"}],
                }
                
                # 发送数据
                yield f"data: {json.dumps(content_chunk)}\n\n".encode()

            # 5. 发送结束标记
            yield "data: [DONE]\n\n".encode()

        except Exception as e:
            # ... (错误处理) ...
            yield "data: [DONE]\n\n".encode()

    return StreamingResponse(stream_generator(), media_type="text/event-stream")
```

### 3. 辅助工具：内容提取
**文件：** `src/openai_transfer.py`

项目中还包含了一个辅助函数，用于从 Gemini 的响应中分离“思考过程”（Thinking/Reasoning）和“实际回答”，这在假流式处理最终结果时被用到。

```python
def _extract_content_and_reasoning(parts: list) -> tuple:
    """从Gemini响应部件中提取内容和推理内容"""
    content = ""
    reasoning_content = ""

    for part in parts:
        # 处理文本内容
        if part.get("text"):
            # 检查这个部件是否包含thinking tokens
            if part.get("thought", False):
                reasoning_content += part.get("text", "")
            else:
                content += part.get("text", "")

    return content, reasoning_content
```

### 总结
要在另一个项目中使用，你需要：
1.  **FastAPI 的 `StreamingResponse`**：用于返回流式响应。
2.  **异步任务机制**：使用 `asyncio.create_task` (或类似封装) 来在后台执行耗时的非流式请求。
3.  **循环等待与心跳**：使用 `while not task.done(): await asyncio.sleep(n)` 循环来维持连接活跃。
4.  **SSE 格式封装**：确保返回的数据符合 Server-Sent Events 规范 (`data: {...}\n\n`)。